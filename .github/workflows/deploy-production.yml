# ============================================================================
# nself-chat Production Deployment Workflow
# ============================================================================
# Deploys to production on release or manual trigger
# Requires approval from production environment
# ============================================================================

name: Deploy to Production

on:
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Docker image tag to deploy (e.g., v1.0.0)'
        required: true
      confirm:
        description: 'Type "deploy" to confirm production deployment'
        required: true

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  NAMESPACE: nself-chat
  RELEASE_NAME: nself-chat

jobs:
  check-secrets:
    name: Check Production Secrets
    runs-on: ubuntu-latest
    outputs:
      has-kubeconfig: ${{ steps.check.outputs.has-kubeconfig }}
    steps:
      - name: Check for Kubernetes secrets
        id: check
        env:
          HAS_KUBECONFIG: ${{ secrets.PRODUCTION_KUBECONFIG != '' }}
        run: |
          echo "has-kubeconfig=${HAS_KUBECONFIG}" >> $GITHUB_OUTPUT
          if [ "${HAS_KUBECONFIG}" != "true" ]; then
            echo "::warning::Production Kubernetes secrets not configured. Deployment will be skipped."
          fi

  validate:
    name: Validate Deployment
    runs-on: ubuntu-latest
    needs: check-secrets
    if: needs.check-secrets.outputs.has-kubeconfig == 'true'

    steps:
      - name: Validate manual trigger
        if: github.event_name == 'workflow_dispatch'
        run: |
          if [ "${{ github.event.inputs.confirm }}" != "deploy" ]; then
            echo "Deployment not confirmed. Please type 'deploy' to proceed."
            exit 1
          fi

      - name: Determine image tag
        id: tag
        run: |
          if [ "${{ github.event_name }}" == "release" ]; then
            echo "tag=${{ github.event.release.tag_name }}" >> $GITHUB_OUTPUT
          else
            echo "tag=${{ github.event.inputs.image_tag }}" >> $GITHUB_OUTPUT
          fi

    outputs:
      image-tag: ${{ steps.tag.outputs.tag }}

  deploy:
    name: Deploy to Production
    needs: validate
    runs-on: ubuntu-latest
    environment:
      name: production
      url: https://chat.nself.io
    concurrency:
      group: production-deploy
      cancel-in-progress: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.28.0'

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: 'v3.13.0'

      - name: Configure kubectl
        env:
          KUBECONFIG_DATA: ${{ secrets.PRODUCTION_KUBECONFIG }}
        run: |
          mkdir -p $HOME/.kube
          echo "$KUBECONFIG_DATA" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Verify cluster connection
        run: kubectl cluster-info

      - name: Verify image exists
        run: |
          docker manifest inspect ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.validate.outputs.image-tag }} > /dev/null

      - name: Create backup annotation
        run: |
          # Store current revision for potential rollback
          CURRENT_REVISION=$(helm history ${{ env.RELEASE_NAME }} -n ${{ env.NAMESPACE }} --max 1 -o json | jq -r '.[0].revision' || echo "0")
          echo "Previous revision: $CURRENT_REVISION"
          echo "PREVIOUS_REVISION=$CURRENT_REVISION" >> $GITHUB_ENV

      - name: Update Helm dependencies
        run: helm dependency update ./deploy/helm/nself-chat

      - name: Deploy to production
        run: |
          helm upgrade --install ${{ env.RELEASE_NAME }} ./deploy/helm/nself-chat \
            --namespace ${{ env.NAMESPACE }} \
            --create-namespace \
            --values ./deploy/helm/nself-chat/values.yaml \
            --values ./deploy/helm/nself-chat/values-production.yaml \
            --set image.tag=${{ needs.validate.outputs.image-tag }} \
            --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
            --wait \
            --timeout 600s \
            --atomic

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/${{ env.RELEASE_NAME }} -n ${{ env.NAMESPACE }} --timeout=300s
          kubectl get pods -n ${{ env.NAMESPACE }} -l app.kubernetes.io/instance=${{ env.RELEASE_NAME }}

      - name: Run health checks
        run: |
          # Wait for DNS propagation
          sleep 30

          # Health check
          for i in {1..5}; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://chat.nself.io/api/health || echo "000")
            if [ "$HTTP_STATUS" == "200" ]; then
              echo "Health check passed!"
              exit 0
            fi
            echo "Health check attempt $i failed with status: $HTTP_STATUS"
            sleep 10
          done
          echo "Health checks failed after 5 attempts"
          exit 1

      - name: Run smoke tests
        run: |
          # Readiness check
          HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://chat.nself.io/api/ready || echo "000")
          if [ "$HTTP_STATUS" != "200" ]; then
            echo "Readiness check failed with status: $HTTP_STATUS"
            exit 1
          fi
          echo "Readiness check passed!"

      - name: Rollback on failure
        if: failure()
        run: |
          echo "Deployment failed! Initiating rollback to revision ${{ env.PREVIOUS_REVISION }}..."
          if [ "${{ env.PREVIOUS_REVISION }}" != "0" ]; then
            helm rollback ${{ env.RELEASE_NAME }} ${{ env.PREVIOUS_REVISION }} -n ${{ env.NAMESPACE }} --wait --timeout 300s
          fi

    outputs:
      deployed-tag: ${{ needs.validate.outputs.image-tag }}

  verify:
    name: Post-deployment Verification
    needs: deploy
    runs-on: ubuntu-latest

    steps:
      - name: Extended health verification
        run: |
          echo "Running extended health verification..."

          # Check multiple endpoints
          ENDPOINTS=("/api/health" "/api/ready")
          for endpoint in "${ENDPOINTS[@]}"; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "https://chat.nself.io${endpoint}" || echo "000")
            if [ "$HTTP_STATUS" != "200" ]; then
              echo "Endpoint ${endpoint} returned ${HTTP_STATUS}"
              exit 1
            fi
            echo "Endpoint ${endpoint} OK"
          done

          echo "All health checks passed!"

  notify:
    name: Notify
    needs: [check-secrets, deploy, verify]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Create deployment notification
        run: |
          if [ "${{ needs.check-secrets.outputs.has-kubeconfig }}" != "true" ]; then
            echo "## Deployment Skipped" >> $GITHUB_STEP_SUMMARY
            echo "Production Kubernetes secrets not configured." >> $GITHUB_STEP_SUMMARY
            echo "To enable production deployment, add PRODUCTION_KUBECONFIG to repository secrets." >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.deploy.result }}" == "success" ] && [ "${{ needs.verify.result }}" == "success" ]; then
            echo "Production deployment successful!"
            echo "Version: ${{ needs.deploy.outputs.deployed-tag }}"
            echo "URL: https://chat.nself.io"
          else
            echo "Production deployment failed or verification failed!"
            echo "Please check the deployment logs."
          fi

      - name: Create GitHub deployment status
        if: always()
        uses: actions/github-script@v8
        with:
          script: |
            const status = '${{ needs.deploy.result }}' === 'success' ? 'success' : 'failure';
            console.log(`Deployment status: ${status}`);
